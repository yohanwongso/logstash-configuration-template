######################################
## LOGSTASH CONFIGURATION TEMPLATE  ##
## Linux Audit Log to Elasticsearch ##
## Version 20230516                 ##
## Â© 2023 Yohan                     ##
######################################
## Tested on SOF-ELK v20230329      ##
######################################

## Log sample:
## type=LOGIN msg=audit(1680738841.009:4232761): pid=100704 uid=0 old-auid=4294967295 auid=1043 tty=(none) old-ses=4294967295 ses=477451 res=1
## type=CRED_DISP msg=audit(1680738786.006:4232757): pid=100280 uid=0 auid=0 ses=477450 msg='op=PAM:setcred grantors=pam_env,pam_tally2,pam_listfile,pam_unix acct="root" exe="/usr/sbin/crond" hostname=? addr=? terminal=cron res=success'

input {
  file {
    ## (Mandatory) Define the file path, can use wildcard "*".
    path => "/mnt/hgfs/D/DFIR/audit/*"

    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
  
  # stdin {}
}

filter {
  mutate {
    remove_field => [ "host", "@version" ]
  }

  grok {
    match => {
      "message" => [
        "^type=%{DATA:[type]} msg=audit\(%{DATA:[epoch_timestamp]}:%{DATA}\): %{DATA:[full_message]}$"
      ]
    }
    tag_on_failure => [ "_grokparsefailure", "grok1_failure" ]
  }
  
  grok {
    match => {
      "[full_message]" => [
        "^%{DATA:[kv1]} msg=\'%{DATA:[msg]}\'$"
      ]
    }
    tag_on_failure => [ "_grokparsefailure", "grok2_failure" ]
  }
    
  if [kv1] {
    kv {
      source => "[kv1]"
      field_split => " "
      value_split => "="
      tag_on_failure => [ "_kv_filter_error", "kv1_failure" ]
    }
  }
  
  if [msg] {
    kv {
      source => "[msg]"
      field_split => " "
      value_split => "="
      tag_on_failure => [ "_kv_filter_error", "kv2_failure" ]
    }
  }
  
  if "grok2_failure" in [tags] {
    kv {
      source => "[full_message]"
      field_split => " "
      value_split => "="
      tag_on_failure => [ "_kv_filter_error", "kv3_failure" ]
    }
  }
  
  date {
    match => [ "[epoch_timestamp]", "UNIX" ]
    timezone => "Asia/Jakarta"
  }
  
  mutate {
    remove_field => [ "kv1" ]
  }
  
  ## (Optional) Uncomment to add tags
  # mutate {
  #   add_tag => [ "tag1", "tag2" ]
  # }

  ## (Optional) Uncomment to add timestamp of the ingestion process by the Logstash.
  # ruby {
  #   code => "event.set('[event][ingested]' , Time.now.utc)"
  # }
}

output {
  ## Uncomment to push the result to Elasticsearch, define the hosts of the elasticsearch and the index name.
  # elasticsearch {
  #   hosts => "localhost:9200"
  #   index => "dfir-01-linux-audit-001"
  # }

  ## Uncomment to print out the result
  # stdout {}
}
